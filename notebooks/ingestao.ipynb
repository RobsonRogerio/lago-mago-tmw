{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6c64989-010b-42f1-900b-84375e771fb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import delta\n",
    "# def table_exists(catalog, database, table):\n",
    "#     count = (\n",
    "#         spark.sql(f\"SHOW TABLES FROM {catalog}.{database}\")\n",
    "#         .filter(f\"database = '{database}' AND tableName = '{table}'\")\n",
    "#         .count()\n",
    "#     )\n",
    "#     return count == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8719c665-38f9-4cb0-90b5-d7e08864785a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Outra abordagem para a função table_exists utilizzando SHOW TABLES IN\n",
    "# import delta\n",
    "# def table_exists(catalog, database, table):\n",
    "#     df_tables = spark.sql(f\"SHOW TABLES IN {catalog}.{database}\")\n",
    "#     # Usando aspas no filtro para evitar erro de parsing\n",
    "#     count = (\n",
    "#         df_tables\n",
    "#         .filter(f\"database = '{database}' AND tableName = '{table}'\")\n",
    "#         .count()\n",
    "#     )\n",
    "#     return count > 0\n",
    "\n",
    "# Vantagem: não precisa tentar acessar a tabela; funciona rápido se o catálogo já está populado.\n",
    "# Observação: se o database na listagem não vier preenchido (isso acontece dependendo da versão/configuração), o filtro pode falhar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc21d84-644c-4eb0-9d6b-2119fdf6d1df",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "# Outra abordagem para a função table_exists usando DESCRIBE TABLE com Try/Except (mais robusto)\n",
    "import delta\n",
    "def table_exists(catalog, database, table):\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE TABLE {catalog}.{database}.{table}\")\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Vantagem: evita depender do formato do SHOW TABLES e do preenchimento da coluna database.\n",
    "# Funciona mesmo se a tabela for externa, view ou Delta table no Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5493359f-06b2-4ca9-af7a-d485e32663f4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup"
    }
   },
   "outputs": [],
   "source": [
    "catalog = 'bronze'\n",
    "schema = 'upsell'\n",
    "tablename = dbutils.widgets.get('tablename')\n",
    "id_field = dbutils.widgets.get('id_field')\n",
    "timestamp_field = dbutils.widgets.get('timestamp_field')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5309e277-df5a-4412-9838-edc36a93a531",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingestão do Full Load"
    }
   },
   "outputs": [],
   "source": [
    "if not table_exists(catalog, schema, tablename):\n",
    "\n",
    "    print('Tabela não existe, criando...')\n",
    "\n",
    "    df_full = spark.read.format('parquet').load(f'/Volumes/raw/full_load/{tablename}/')\n",
    "    \n",
    "    (df_full.coalesce(1)\n",
    "    .write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\n",
    "    f\"{catalog}.{schema}.{tablename}\")\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print('Tabela já existe, ignorando full-load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f65501a-a8b4-4057-a982-8e3aec1f8852",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"IdCliente\":262},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755173244840}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Leitura do CDC"
    }
   },
   "outputs": [],
   "source": [
    "spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(f\"/Volumes/raw/data/cdc/{tablename}/\") \\\n",
    "    .createOrReplaceTempView(f\"view_{tablename}\")\n",
    "\n",
    "query = f\"\"\"\n",
    "    select *\n",
    "    from view_{tablename}\n",
    "    qualify row_number() over (partition by {id_field} order by {timestamp_field} desc) = 1\n",
    "\"\"\"\n",
    "\n",
    "df_cdc_unique = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f52133cd-f75c-4fb3-9913-e573cb8c7d32",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Escrita do CDC (upsert)"
    }
   },
   "outputs": [],
   "source": [
    "bronze = delta.DeltaTable.forName(spark, f'{catalog}.{schema}.{tablename}')\n",
    "\n",
    "bronze.alias('b') \\\n",
    "        .merge(df_cdc_unique.alias('d'), f'b.{id_field} = d.{id_field}') \\\n",
    "        .whenMatchedDelete(condition=\"d.op = 'D'\") \\\n",
    "        .whenMatchedUpdateAll(condition=\"d.op = 'U'\") \\\n",
    "        .whenNotMatchedInsertAll(condition=\"d.op = 'I' OR d.op = 'U'\") \\\n",
    "        .execute()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8887589874606071,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestao",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
