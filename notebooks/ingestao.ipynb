{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6c64989-010b-42f1-900b-84375e771fb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import delta\n",
    "# def table_exists(catalog, database, table):\n",
    "#     count = (\n",
    "#         spark.sql(f\"SHOW TABLES FROM {catalog}.{database}\")\n",
    "#         .filter(f\"database = '{database}' AND tableName = '{table}'\")\n",
    "#         .count()\n",
    "#     )\n",
    "#     return count == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8719c665-38f9-4cb0-90b5-d7e08864785a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Outra abordagem para a função table_exists utilizzando SHOW TABLES IN\n",
    "# import delta\n",
    "# def table_exists(catalog, database, table):\n",
    "#     df_tables = spark.sql(f\"SHOW TABLES IN {catalog}.{database}\")\n",
    "#     # Usando aspas no filtro para evitar erro de parsing\n",
    "#     count = (\n",
    "#         df_tables\n",
    "#         .filter(f\"database = '{database}' AND tableName = '{table}'\")\n",
    "#         .count()\n",
    "#     )\n",
    "#     return count > 0\n",
    "\n",
    "# Vantagem: não precisa tentar acessar a tabela; funciona rápido se o catálogo já está populado.\n",
    "# Observação: se o database na listagem não vier preenchido (isso acontece dependendo da versão/configuração), o filtro pode falhar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc21d84-644c-4eb0-9d6b-2119fdf6d1df",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "# Outra abordagem para a função table_exists usando DESCRIBE TABLE com Try/Except (mais robusto)\n",
    "\n",
    "import delta\n",
    "def table_exists(catalog, database, table):\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE TABLE {catalog}.{database}.{table}\")\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Vantagem: evita depender do formato do SHOW TABLES e do preenchimento da coluna database.\n",
    "# Funciona mesmo se a tabela for externa, view ou Delta table no Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5493359f-06b2-4ca9-af7a-d485e32663f4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup"
    }
   },
   "outputs": [],
   "source": [
    "catalog = 'bronze'\n",
    "schema = 'upsell'\n",
    "# tablename = dbutils.widgets.get('tablename')\n",
    "# id_field = dbutils.widgets.get('id_field')\n",
    "# timestamp_field = dbutils.widgets.get('timestamp_field')\n",
    "tablename = 'customers'\n",
    "id_field = 'IdCliente'\n",
    "timestamp_field = 'DtAtualizacao'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5309e277-df5a-4412-9838-edc36a93a531",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingestão do Full Load"
    }
   },
   "outputs": [],
   "source": [
    "if not table_exists(catalog, schema, tablename):\n",
    "\n",
    "    print('Tabela não existe, criando...')\n",
    "\n",
    "    df_full = spark.read.format('parquet').load(f'/Volumes/raw/full_load/{tablename}/')\n",
    "    \n",
    "    (df_full.coalesce(1)\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(f\"{catalog}.{schema}.{tablename}\"))\n",
    "\n",
    "else:\n",
    "    print('Tabela já existe, ignorando full-load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfddb3b9-e41e-41e0-af66-43bb37959315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Capturando o schema da tabela\n",
    "df_full = spark.read.format('parquet').load(f'/Volumes/raw/data/cdc/{tablename}/')\n",
    "schema = df_full.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f65501a-a8b4-4057-a982-8e3aec1f8852",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Leitura do CDC"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "bronze = delta.DeltaTable.forName(spark, f'{catalog}.{schema}.{tablename}')\n",
    "\n",
    "def upsert(df,deltatable):      \n",
    "      # Define a janela (window) para row_number\n",
    "      window_spec = Window.partitionBy(id_field).orderBy(F.col(timestamp_field).desc())\n",
    "\n",
    "      # Adiciona o row_number\n",
    "      df_cdc = df.withColumn(\"row_num\", F.row_number().over(window_spec)) \\\n",
    "                  .filter(F.col(\"row_num\") == 1) \\\n",
    "                  .drop(\"row_num\")\n",
    "\n",
    "      deltatable.alias('b') \\\n",
    "            .merge(df_cdc.alias('d'), f'b.{id_field} = d.{id_field}') \\\n",
    "            .whenMatchedDelete(condition=\"d.op = 'D'\") \\\n",
    "            .whenMatchedUpdateAll(condition=\"d.op = 'U'\") \\\n",
    "            .whenNotMatchedInsertAll(condition=\"d.op = 'I' OR d.op = 'U'\") \\\n",
    "            .execute()\n",
    "\n",
    "df_stream = spark.readStream \\\n",
    "      .format(\"cloudFiles\") \\\n",
    "      .option(\"cloudFiles.format\", \"parquet\") \\\n",
    "      .schema(schema) \\\n",
    "      .load(f\"/Volumes/raw/data/cdc/{tablename}/\")\n",
    "\n",
    "stream = df_stream.writeStream \\\n",
    "      .option(\"checkpointLocation\", f\"/Volumes/raw/data/cdc/{tablename}_checkpoints\") \\\n",
    "      .foreachBatch(lambda df, BatchId: upsert(df,bronze)) \\\n",
    "      .trigger(availableNow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "481f5514-04d5-41c9-af53-524cfcc38239",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Leitura do CDC sugerida pelo Assistant"
    }
   },
   "outputs": [],
   "source": [
    "# Opção sugerida pelo Assistant do Databricks:\n",
    "# You need to specify the cloudFiles.schemaLocation option to enable schema inference and evolution when using Auto Loader. This option points to a directory where Databricks will store schema information for your stream. Here is the fixed code:\n",
    "\n",
    "# df_stream = (\n",
    "#     spark.readStream\n",
    "#     .format(\"cloudFiles\")\n",
    "#     .option(\"cloudFiles.format\", \"parquet\")\n",
    "#     .option(\"cloudFiles.schemaLocation\", f\"/Volumes/raw/data/cdc/{tablename}/_schemas\")\n",
    "#     .load(f\"/Volumes/raw/data/cdc/{tablename}/\")\n",
    "# )\n",
    "\n",
    "# This code adds the required schemaLocation option. The directory specified will track schema changes over time for your streaming source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f52133cd-f75c-4fb3-9913-e573cb8c7d32",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Escrita do CDC (upsert)"
    }
   },
   "outputs": [],
   "source": [
    "start = stream.start()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8887589874606071,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestao",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
