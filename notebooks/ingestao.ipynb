{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6c64989-010b-42f1-900b-84375e771fb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import delta\n",
    "# def table_exists(catalog, database, table):\n",
    "#     count = (\n",
    "#         spark.sql(f\"SHOW TABLES FROM {catalog}.{database}\")\n",
    "#         .filter(f\"database = '{database}' AND tableName = '{table}'\")\n",
    "#         .count()\n",
    "#     )\n",
    "#     return count == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8719c665-38f9-4cb0-90b5-d7e08864785a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Outra abordagem para a função table_exists utilizzando SHOW TABLES IN\n",
    "# import delta\n",
    "# def table_exists(catalog, database, table):\n",
    "#     df_tables = spark.sql(f\"SHOW TABLES IN {catalog}.{database}\")\n",
    "#     # Usando aspas no filtro para evitar erro de parsing\n",
    "#     count = (\n",
    "#         df_tables\n",
    "#         .filter(f\"database = '{database}' AND tableName = '{table}'\")\n",
    "#         .count()\n",
    "#     )\n",
    "#     return count > 0\n",
    "\n",
    "# Vantagem: não precisa tentar acessar a tabela; funciona rápido se o catálogo já está populado.\n",
    "# Observação: se o database na listagem não vier preenchido (isso acontece dependendo da versão/configuração), o filtro pode falhar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc21d84-644c-4eb0-9d6b-2119fdf6d1df",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "# Outra abordagem para a função table_exists usando DESCRIBE TABLE com Try/Except (mais robusto)\n",
    "\n",
    "import delta\n",
    "def table_exists(catalog, database, table):\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE TABLE {catalog}.{database}.{table}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Vantagem: evita depender do formato do SHOW TABLES e do preenchimento da coluna database.\n",
    "# Funciona mesmo se a tabela for externa, view ou Delta table no Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5493359f-06b2-4ca9-af7a-d485e32663f4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup"
    }
   },
   "outputs": [],
   "source": [
    "catalog = 'bronze'\n",
    "schema = 'upsell'\n",
    "tablename = dbutils.widgets.get('tablename')\n",
    "id_field = dbutils.widgets.get('id_field')\n",
    "timestamp_field = dbutils.widgets.get('timestamp_field')\n",
    "# tablename = 'customers'\n",
    "# id_field = 'IdCliente'\n",
    "# timestamp_field = 'DtAtualizacao'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5309e277-df5a-4412-9838-edc36a93a531",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingestão do Full Load"
    }
   },
   "outputs": [],
   "source": [
    "if not table_exists(catalog, schema, tablename):\n",
    "\n",
    "    print('Tabela não existe, criando...')\n",
    "\n",
    "    df_full = spark.read.format('parquet').load(f'/Volumes/raw/full_load/{tablename}/')\n",
    "    \n",
    "    (df_full.coalesce(1)\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(f\"{catalog}.{schema}.{tablename}\"))\n",
    "\n",
    "else:\n",
    "    print('Tabela já existe, ignorando full-load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfddb3b9-e41e-41e0-af66-43bb37959315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Capturando o schema da tabela\n",
    "df_full = spark.read.format('parquet').load(f'/Volumes/raw/data/cdc/{tablename}/')\n",
    "schema_df = df_full.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f65501a-a8b4-4057-a982-8e3aec1f8852",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Leitura do CDC"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "bronze = delta.DeltaTable.forName(spark, f'{catalog}.{schema}.{tablename}')\n",
    "\n",
    "def upsert(df, table_name):\n",
    "    df.createOrReplaceTempView(\"cdc_temp_view\")\n",
    "\n",
    "    merge_sql = f\"\"\"\n",
    "    MERGE INTO {table_name} AS b\n",
    "    USING (\n",
    "        SELECT * FROM cdc_temp_view\n",
    "        QUALIFY row_number() OVER (PARTITION BY {id_field} ORDER BY {timestamp_field} DESC) = 1\n",
    "    ) AS d\n",
    "    ON b.{id_field} = d.{id_field}\n",
    "    WHEN MATCHED AND d.op = 'D' THEN DELETE\n",
    "    WHEN MATCHED AND d.op = 'U' THEN UPDATE SET *\n",
    "    WHEN NOT MATCHED AND (d.op = 'I' OR d.op = 'U') THEN INSERT *\n",
    "    \"\"\"\n",
    "\n",
    "    spark.sql(merge_sql)\n",
    "\n",
    "\n",
    "df_stream = spark.readStream \\\n",
    "      .format(\"cloudFiles\") \\\n",
    "      .option(\"cloudFiles.format\", \"parquet\") \\\n",
    "      .schema(schema_df) \\\n",
    "      .load(f\"/Volumes/raw/data/cdc/{tablename}/\")\n",
    "\n",
    "stream = df_stream.writeStream \\\n",
    "      .option(\"checkpointLocation\", f\"/Volumes/raw/data/cdc/{tablename}_checkpoints\") \\\n",
    "      .foreachBatch(lambda df, BatchId: upsert(df, f'{catalog}.{schema}.{tablename}')) \\\n",
    "      .trigger(availableNow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "481f5514-04d5-41c9-af53-524cfcc38239",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Leitura do CDC sugerida pelo Assistant"
    }
   },
   "outputs": [],
   "source": [
    "# Opção sugerida pelo Assistant do Databricks:\n",
    "# You need to specify the cloudFiles.schemaLocation option to enable schema inference and evolution when using Auto Loader. This option points to a directory where Databricks will store schema information for your stream. Here is the fixed code:\n",
    "\n",
    "# df_stream = (\n",
    "#     spark.readStream\n",
    "#     .format(\"cloudFiles\")\n",
    "#     .option(\"cloudFiles.format\", \"parquet\")\n",
    "#     .option(\"cloudFiles.schemaLocation\", f\"/Volumes/raw/data/cdc/{tablename}/_schemas\")\n",
    "#     .load(f\"/Volumes/raw/data/cdc/{tablename}/\")\n",
    "# )\n",
    "\n",
    "# This code adds the required schemaLocation option. The directory specified will track schema changes over time for your streaming source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f52133cd-f75c-4fb3-9913-e573cb8c7d32",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Escrita do CDC (upsert)"
    }
   },
   "outputs": [],
   "source": [
    "start = stream.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ae128e6-79fa-43ed-bbd9-c13ba7231ff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5be95c0-e244-49c5-95e7-7fcdad949cd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "985bce30-c8e6-42b8-8eac-7941dc56ce44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestao",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
