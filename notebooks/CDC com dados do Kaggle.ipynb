{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b59583df-4c21-4653-aebe-f3d7afc020f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install kaggle pandas python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d919ede3-88e8-455d-a719-ba943be4a4b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/Volumes/raw/data/configs_volume\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9b86575-2b1c-4fde-835c-016d659ed257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with open(\"/Volumes/raw/data/configs_volume/config.json\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3feec09f-3fb6-4a7f-9a6b-e7e30f6ddd34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ==== 1. Credenciais Kaggle via Secret Scope ====\n",
    "KAGGLE_USERNAME = dbutils.secrets.get(scope=\"kaggle_scope\", key=\"KAGGLE_USERNAME\")\n",
    "KAGGLE_KEY = dbutils.secrets.get(scope=\"kaggle_scope\", key=\"KAGGLE_KEY\")\n",
    "\n",
    "# Variáveis de ambiente para a API Kaggle\n",
    "os.environ[\"KAGGLE_USERNAME\"] = KAGGLE_USERNAME\n",
    "os.environ[\"KAGGLE_KEY\"] = KAGGLE_KEY\n",
    "\n",
    "# ==== 2. Caminhos no S3/Unity Catalog ====\n",
    "BASE_PATH = \"/Volumes/raw/data\"        # schema data\n",
    "ACTUAL_PATH = f\"{BASE_PATH}/actual\"\n",
    "LAST_PATH = f\"{BASE_PATH}/last\"\n",
    "\n",
    "CDC_BASE_PATH = \"/Volumes/raw/cdc\"     # schema cdc (volumes por tabela)\n",
    "\n",
    "# ==== 3. Config JSON (salvo também no volume) ====\n",
    "CONFIG_PATH = \"/Volumes/raw/data/configs_volume/config.json\"\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "# ==== 4. Funções CDC ====\n",
    "def get_update_lines(df_last, df_actual, pk, date_field):\n",
    "    df_update = df_last.merge(\n",
    "        df_actual,\n",
    "        how=\"left\",\n",
    "        on=[pk],\n",
    "        suffixes=('_x', '_y')\n",
    "    )\n",
    "    update_flag = df_update[date_field + '_y'] > df_update[date_field + '_x']\n",
    "    ids_updated = df_update[update_flag][pk].tolist()\n",
    "    df_update = df_actual[df_actual[pk].isin(ids_updated)].copy()\n",
    "    df_update[\"op\"] = \"U\"\n",
    "    return df_update\n",
    "\n",
    "def get_insert_lines(df_last, df_actual, pk):\n",
    "    df_insert = df_actual[~df_actual[pk].isin(df_last[pk])].copy()\n",
    "    df_insert[\"op\"] = \"I\"\n",
    "    return df_insert\n",
    "\n",
    "def get_delete_lines(df_last, df_actual, pk):\n",
    "    df_delete = df_last[~df_last[pk].isin(df_actual[pk])].copy()\n",
    "    df_delete[\"op\"] = \"D\"\n",
    "    return df_delete\n",
    "\n",
    "def create_cdc(df_last, df_actual, pk, date_field):\n",
    "    df_update = get_update_lines(df_last, df_actual, pk, date_field)\n",
    "    df_insert = get_insert_lines(df_last, df_actual, pk)\n",
    "    df_delete = get_delete_lines(df_last, df_actual, pk)\n",
    "    return pd.concat([df_update, df_insert, df_delete], ignore_index=True)\n",
    "\n",
    "# ==== 5. Processamento CDC - Parquet por tabela com histórico ====\n",
    "def process_cdc(tables):\n",
    "    print(\"Processando CDC de todas as tabelas...\")\n",
    "    for t in tables:\n",
    "        file_last = f\"{LAST_PATH}/{t['name']}.csv\"\n",
    "        file_actual = f\"{ACTUAL_PATH}/{t['name']}.csv\"\n",
    "\n",
    "        try:\n",
    "            df_last = pd.read_csv(file_last, sep=t[\"sep\"], on_bad_lines='skip')\n",
    "        except Exception as e:\n",
    "            print(f\"[ERRO] Falha ao ler {file_last}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df_actual = pd.read_csv(file_actual, sep=t[\"sep\"], on_bad_lines='skip')\n",
    "        except Exception as e:\n",
    "            print(f\"[ERRO] Falha ao ler {file_actual}: {e}\")\n",
    "            continue\n",
    "\n",
    "        df_cdc = create_cdc(df_last, df_actual, t[\"pk\"], t[\"date_field\"])\n",
    "\n",
    "        if df_cdc.empty:\n",
    "            print(f\"Nenhuma alteração encontrada para {t['name']}.\")\n",
    "            continue\n",
    "\n",
    "        # Caminho do volume específico da tabela CDC\n",
    "        cdc_path = f\"{CDC_BASE_PATH}/{t['name']}\"\n",
    "        dbutils.fs.mkdirs(cdc_path)\n",
    "\n",
    "        # Timestamp para nome do arquivo\n",
    "        now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        parquet_path = f\"{cdc_path}/{now}.parquet\"\n",
    "\n",
    "        # Salva como Parquet (Spark necessário para gravar em UC/S3)\n",
    "        df_spark = spark.createDataFrame(df_cdc)\n",
    "        df_spark.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "\n",
    "        print(f\"[LOG] CDC de {t['name']} salvo em {parquet_path}\")\n",
    "\n",
    "    print(\"CDC processado com sucesso!\")\n",
    "\n",
    "# ==== 6. Baixar dados Kaggle ====\n",
    "def download_kaggle_dataset(dataset_name):\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    import tempfile\n",
    "    import os\n",
    "\n",
    "    # Cria diretório temporário local seguro no driver do cluster\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "\n",
    "        print(f\"Baixando dataset {dataset_name} para {tmpdir}...\")\n",
    "        api.dataset_download_files(dataset_name, path=tmpdir, unzip=True)\n",
    "        print(\"Download concluído!\")\n",
    "\n",
    "        # Garante que o destino existe no volume S3\n",
    "        dbutils.fs.mkdirs(ACTUAL_PATH)\n",
    "\n",
    "        # Copia arquivos da pasta temporária local para o volume montado no S3\n",
    "        for item in os.listdir(tmpdir):\n",
    "            local_file = os.path.join(tmpdir, item)\n",
    "            target_path = f\"{ACTUAL_PATH}/{item}\"\n",
    "            dbutils.fs.cp(f\"file:{local_file}\", target_path)\n",
    "\n",
    "        print(f\"Arquivos movidos para {ACTUAL_PATH}\")\n",
    "\n",
    "\n",
    "# ==== 7. Mover actual -> last ====\n",
    "def move_from_actual_to_last():\n",
    "    dbutils.fs.mkdirs(LAST_PATH)\n",
    "    print(f\"Movendo arquivos de {ACTUAL_PATH} para {LAST_PATH}...\")\n",
    "    files = dbutils.fs.ls(ACTUAL_PATH)\n",
    "    for f in files:\n",
    "        dbutils.fs.mv(f.path, f\"{LAST_PATH}/{os.path.basename(f.path)}\", True)\n",
    "    print(\"Arquivos movidos com sucesso!\")\n",
    "\n",
    "# ==== 8. Main ====\n",
    "def main():\n",
    "    dataset_name = CONFIG[\"dataset_name\"]\n",
    "    move_from_actual_to_last()\n",
    "    download_kaggle_dataset(dataset_name)\n",
    "    process_cdc(CONFIG[\"tables\"])\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22e4e125-5436-4aa9-9d55-0e9b58d4cc52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/tmp\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CDC com dados do Kaggle",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
