{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "757ce877-e76b-4302-af8e-64a8d17f67a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d6c64989-010b-42f1-900b-84375e771fb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import delta\n",
    "# def table_exists(catalog, database, table):\n",
    "#     count = (\n",
    "#         spark.sql(f\"SHOW TABLES FROM {catalog}.{database}\")\n",
    "#         .filter(f\"database = '{database}' AND tableName = '{table}'\")\n",
    "#         .count()\n",
    "#     )\n",
    "#     return count == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8719c665-38f9-4cb0-90b5-d7e08864785a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Outra abordagem para a função table_exists utilizando SHOW TABLES IN\n",
    "# import delta\n",
    "# def table_exists(catalog, database, table):\n",
    "#     df_tables = spark.sql(f\"SHOW TABLES IN {catalog}.{database}\")\n",
    "#     # Usando aspas no filtro para evitar erro de parsing\n",
    "#     count = (\n",
    "#         df_tables\n",
    "#         .filter(f\"database = '{database}' AND tableName = '{table}'\")\n",
    "#         .count()\n",
    "#     )\n",
    "#     return count > 0\n",
    "\n",
    "# Vantagem: não precisa tentar acessar a tabela; funciona rápido se o catálogo já está populado.\n",
    "# Observação: se o database na listagem não vier preenchido (isso acontece dependendo da versão/configuração), o filtro pode falhar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc21d84-644c-4eb0-9d6b-2119fdf6d1df",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "import delta\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../lib')\n",
    "\n",
    "import utils\n",
    "import ingestors\n",
    "from pyspark.sql.functions import col, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5493359f-06b2-4ca9-af7a-d485e32663f4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "catalog = 'bronze'\n",
    "schemaname = 'upsell'\n",
    "tablename = dbutils.widgets.get('tablename')\n",
    "id_field = dbutils.widgets.get('id_field')\n",
    "timestamp_field = dbutils.widgets.get('timestamp_field')\n",
    "\n",
    "full_load_path = f\"/Volumes/raw/{schemaname}/full_load/{tablename}/\"\n",
    "cdc_path = f\"/Volumes/raw/{schemaname}/cdc/{tablename}/\"\n",
    "checkpoint_location = f\"/Volumes/raw/{schemaname}/cdc/{tablename}_checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5309e277-df5a-4412-9838-edc36a93a531",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingestão do Full Load"
    }
   },
   "outputs": [],
   "source": [
    "if not utils.table_exists(spark, catalog, schemaname, tablename):\n",
    "\n",
    "    print('Tabela não existe, criando...')\n",
    "\n",
    "    dbutils.fs.rm(checkpoint_location, True)\n",
    "    \n",
    "    ingest_full_load = ingestors.ingestor(spark=spark, \n",
    "                                        catalog=catalog,\n",
    "                                        schemaname=schemaname, \n",
    "                                        tablename=tablename, \n",
    "                                        data_format='parquet')\n",
    "    ingest_full_load.execute(full_load_path)\n",
    "\n",
    "    print(f'Criando tabela {tablename} em {catalog}.{schemaname}. Tabela criada com sucesso!')\n",
    "\n",
    "else:\n",
    "    print(f'Tabela {tablename} já existente em {catalog}.{schemaname}, ignorando full-load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f3c9c3c-37af-4444-87b9-4c192f8870f2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CDC"
    }
   },
   "outputs": [],
   "source": [
    "ingest_cdc = ingestors.ingestorCDC(spark=spark,\n",
    "                                  catalog=catalog,\n",
    "                                  schemaname=schemaname, \n",
    "                                  tablename=tablename, \n",
    "                                  data_format='parquet',\n",
    "                                  id_field=id_field,\n",
    "                                  timestamp_field=timestamp_field)\n",
    "\n",
    "stream = ingest_cdc.execute(cdc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0f65501a-a8b4-4057-a982-8e3aec1f8852",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Leitura do CDC"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.window import Window\n",
    "# from pyspark.sql import functions as F\n",
    "# data_schema = utils.import_schema(tablename)\n",
    "# bronze = delta.DeltaTable.forName(spark, f'{catalog}.{schemaname}.{tablename}')\n",
    "\n",
    "# def upsert(df, table_name):\n",
    "#     df.createOrReplaceTempView(\"cdc_temp_view\")\n",
    "\n",
    "#     merge_sql = f\"\"\"\n",
    "#     MERGE INTO {tablename} AS b\n",
    "#     USING (\n",
    "#         SELECT * FROM cdc_temp_view\n",
    "#         QUALIFY row_number() OVER (PARTITION BY {id_field} ORDER BY {timestamp_field} DESC) = 1\n",
    "#     ) AS d\n",
    "#     ON b.{id_field} = d.{id_field}\n",
    "#     WHEN MATCHED AND d.op = 'D' THEN DELETE\n",
    "#     WHEN MATCHED AND d.op = 'U' THEN UPDATE SET *\n",
    "#     WHEN NOT MATCHED AND (d.op = 'I' OR d.op = 'U') THEN INSERT *\n",
    "#     \"\"\"\n",
    "\n",
    "#     spark.sql(merge_sql)\n",
    "\n",
    "\n",
    "# df_stream = spark.readStream \\\n",
    "#       .format(\"cloudFiles\") \\\n",
    "#       .option(\"cloudFiles.format\", \"parquet\") \\\n",
    "#       .schema(data_schema) \\\n",
    "#       .load(f\"/Volumes/raw/data/cdc/{tablename}/\")\n",
    "\n",
    "# stream = df_stream.writeStream \\\n",
    "#       .option(\"checkpointLocation\", f\"/Volumes/raw/data/cdc/{tablename}_checkpoints\") \\\n",
    "#       .foreachBatch(lambda df, BatchId: upsert(df, f'{catalog}.{schemaname}.{tablename}')) \\\n",
    "#       .trigger(availableNow=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6823144141918004,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestao",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
