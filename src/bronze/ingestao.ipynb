{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6c64989-010b-42f1-900b-84375e771fb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import delta\n",
    "# def table_exists(catalog, database, table):\n",
    "#     count = (\n",
    "#         spark.sql(f\"SHOW TABLES FROM {catalog}.{database}\")\n",
    "#         .filter(f\"database = '{database}' AND tableName = '{table}'\")\n",
    "#         .count()\n",
    "#     )\n",
    "#     return count == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8719c665-38f9-4cb0-90b5-d7e08864785a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Outra abordagem para a função table_exists utilizando SHOW TABLES IN\n",
    "# import delta\n",
    "# def table_exists(catalog, database, table):\n",
    "#     df_tables = spark.sql(f\"SHOW TABLES IN {catalog}.{database}\")\n",
    "#     # Usando aspas no filtro para evitar erro de parsing\n",
    "#     count = (\n",
    "#         df_tables\n",
    "#         .filter(f\"database = '{database}' AND tableName = '{table}'\")\n",
    "#         .count()\n",
    "#     )\n",
    "#     return count > 0\n",
    "\n",
    "# Vantagem: não precisa tentar acessar a tabela; funciona rápido se o catálogo já está populado.\n",
    "# Observação: se o database na listagem não vier preenchido (isso acontece dependendo da versão/configuração), o filtro pode falhar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc21d84-644c-4eb0-9d6b-2119fdf6d1df",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "import delta\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../lib')\n",
    "\n",
    "import utils\n",
    "from pyspark.sql.functions import col, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5493359f-06b2-4ca9-af7a-d485e32663f4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "catalog = 'bronze'\n",
    "schemaname = 'upsell'\n",
    "# tablename = dbutils.widgets.get('tablename')\n",
    "# id_field = dbutils.widgets.get('id_field')\n",
    "# timestamp_field = dbutils.widgets.get('timestamp_field')\n",
    "tablename = 'transactions'\n",
    "id_field = 'IdTransacao'\n",
    "timestamp_field = 'DtCriacao'\n",
    "\n",
    "checkpoint_location = f\"/Volumes/raw/{schemaname}/cdc/{tablename}_checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dae2c8fa-92c2-442a-ba96-a528843bc3e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ingestor:\n",
    "\n",
    "    def __init__(self, catalog, schemaname, tablename, data_format):\n",
    "        self.catalog = catalog\n",
    "        self.schemaname = schemaname\n",
    "        self.tablename = tablename\n",
    "        self.format = data_format\n",
    "        self.set_schema()\n",
    "\n",
    "    def set_schema(self):\n",
    "        self.data_schema = utils.import_schema(self.tablename)\n",
    "    \n",
    "    def load(self, path):\n",
    "        df = spark.read\\\n",
    "                .format(self.format)\\\n",
    "                .schema(self.data_schema)\\\n",
    "                .load(path)\n",
    "        # Converte a coluna de timestamp dinamicamente\n",
    "        df = df.withColumn(self.timestamp_field, to_timestamp(col(self.timestamp_field), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def save(self, df):\n",
    "        (df.coalesce(1)\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .saveAsTable(f\"{self.catalog}.{self.schemaname}.{self.tablename}\"))\n",
    "        return True\n",
    "        \n",
    "    def execute(self, path):\n",
    "        df = self.load(path)\n",
    "        return self.save(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1df59c1-f9c2-4b87-83d4-0e71fe5f3d33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ingestorCDC(ingestor):\n",
    "\n",
    "    def __init__(self, catalog, schemaname, tablename, data_format, id_field, timestamp_field):\n",
    "        super().__init__(catalog, schemaname, tablename, data_format)\n",
    "        self.id_field = id_field\n",
    "        self.timestamp_field = timestamp_field\n",
    "        self.set_deltatable()\n",
    "        \n",
    "    def set_deltatable(self):\n",
    "        tablename = f\"{self.catalog}.{self.schemaname}.{self.tablename}\"\n",
    "        self.deltatable = delta.DeltaTable.forName(spark, tablename)\n",
    "\n",
    "    def upsert(self, df, BatchId=None):\n",
    "        df.createOrReplaceTempView(\"cdc_temp_view\")\n",
    "\n",
    "        table_full_name = f\"{self.catalog}.{self.schemaname}.{self.tablename}\"\n",
    "        merge_sql = f\"\"\"\n",
    "        MERGE INTO {table_full_name} AS b\n",
    "        USING (\n",
    "            SELECT * FROM cdc_temp_view\n",
    "            QUALIFY row_number() OVER (PARTITION BY {self.id_field} ORDER BY {self.timestamp_field} DESC) = 1\n",
    "        ) AS d\n",
    "        ON b.{self.id_field} = d.{self.id_field}\n",
    "        WHEN MATCHED AND d.op = 'D' THEN DELETE\n",
    "        WHEN MATCHED AND d.op = 'U' THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED AND (d.op = 'I' OR d.op = 'U') THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        spark.sql(merge_sql)\n",
    "\n",
    "    def load(self, path):\n",
    "        df = spark.readStream \\\n",
    "                    .format(\"cloudFiles\") \\\n",
    "                    .option(\"cloudFiles.format\", self.format) \\\n",
    "                    .schema(self.data_schema) \\\n",
    "                    .load(path)\n",
    "        return df\n",
    "    \n",
    "    def save(self, df):\n",
    "        stream = df.writeStream \\\n",
    "                    .option(\"checkpointLocation\", f\"/Volumes/raw/{self.schemaname}/cdc/{self.tablename}_checkpoints\") \\\n",
    "                    .foreachBatch(lambda df, _: self.upsert(df)) \\\n",
    "                    .trigger(availableNow=True)\n",
    "        return stream.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5309e277-df5a-4412-9838-edc36a93a531",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingestão do Full Load"
    }
   },
   "outputs": [],
   "source": [
    "if not utils.table_exists(spark, catalog, schemaname, tablename):\n",
    "\n",
    "    print('Tabela não existe, criando...')\n",
    "\n",
    "    dbutils.fs.rm(checkpoint_location, True)\n",
    "    \n",
    "    ingest_full_load = ingestor(catalog=catalog,\n",
    "                                schemaname=schemaname, \n",
    "                                tablename=tablename, \n",
    "                                data_format='parquet')\n",
    "    ingest_full_load.execute(f\"/Volumes/raw/{schemaname}/full_load/{tablename}/\")\n",
    "\n",
    "    print('Tabela criada com sucesso!')\n",
    "\n",
    "else:\n",
    "    print('Tabela já existe, ignorando full-load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f3c9c3c-37af-4444-87b9-4c192f8870f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingest_cdc = ingestorCDC(catalog=catalog,\n",
    "                         schemaname=schemaname, \n",
    "                         tablename=tablename, \n",
    "                         data_format='parquet',\n",
    "                         id_field=id_field,\n",
    "                         timestamp_field=timestamp_field)\n",
    "\n",
    "stream = ingest_cdc.execute(f\"/Volumes/raw/{schemaname}/cdc/{tablename}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27e17278-5f64-4c07-aa89-c34f860c697f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"col_name\":175},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756579190621}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE bronze.upsell.transactions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54138e65-dd32-4bbd-9caa-4ed2faf39ea0",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"IdCliente\":399,\"DtCriacao\":309},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756152046117}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from bronze.upsell.customers\n",
    "order by DtAtualizacao desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f65501a-a8b4-4057-a982-8e3aec1f8852",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Leitura do CDC"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.window import Window\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# bronze = delta.DeltaTable.forName(spark, f'{catalog}.{schema}.{tablename}')\n",
    "\n",
    "# def upsert(df, table_name):\n",
    "#     df.createOrReplaceTempView(\"cdc_temp_view\")\n",
    "\n",
    "#     merge_sql = f\"\"\"\n",
    "#     MERGE INTO {table_name} AS b\n",
    "#     USING (\n",
    "#         SELECT * FROM cdc_temp_view\n",
    "#         QUALIFY row_number() OVER (PARTITION BY {id_field} ORDER BY {timestamp_field} DESC) = 1\n",
    "#     ) AS d\n",
    "#     ON b.{id_field} = d.{id_field}\n",
    "#     WHEN MATCHED AND d.op = 'D' THEN DELETE\n",
    "#     WHEN MATCHED AND d.op = 'U' THEN UPDATE SET *\n",
    "#     WHEN NOT MATCHED AND (d.op = 'I' OR d.op = 'U') THEN INSERT *\n",
    "#     \"\"\"\n",
    "\n",
    "#     spark.sql(merge_sql)\n",
    "\n",
    "\n",
    "# df_stream = spark.readStream \\\n",
    "#       .format(\"cloudFiles\") \\\n",
    "#       .option(\"cloudFiles.format\", \"parquet\") \\\n",
    "#       .schema(df_schema) \\\n",
    "#       .load(f\"/Volumes/raw/data/cdc/{tablename}/\")\n",
    "\n",
    "# stream = df_stream.writeStream \\\n",
    "#       .option(\"checkpointLocation\", f\"/Volumes/raw/data/cdc/{tablename}_checkpoints\") \\\n",
    "#       .foreachBatch(lambda df, BatchId: upsert(df, f'{catalog}.{schema}.{tablename}')) \\\n",
    "#       .trigger(availableNow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "481f5514-04d5-41c9-af53-524cfcc38239",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Leitura do CDC sugerida pelo Assistant"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Opção sugerida pelo Assistant do Databricks:\n",
    "# You need to specify the cloudFiles.schemaLocation option to enable schema inference and evolution when using Auto Loader. This option points to a directory where Databricks will store schema information for your stream. Here is the fixed code:\n",
    "\n",
    "# df_stream = (\n",
    "#     spark.readStream\n",
    "#     .format(\"cloudFiles\")\n",
    "#     .option(\"cloudFiles.format\", \"parquet\")\n",
    "#     .option(\"cloudFiles.schemaLocation\", f\"/Volumes/raw/data/cdc/{tablename}/_schemas\")\n",
    "#     .load(f\"/Volumes/raw/data/cdc/{tablename}/\")\n",
    "# )\n",
    "\n",
    "# This code adds the required schemaLocation option. The directory specified will track schema changes over time for your streaming source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f52133cd-f75c-4fb3-9913-e573cb8c7d32",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Escrita do CDC (upsert)"
    }
   },
   "outputs": [],
   "source": [
    "start = stream.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ae128e6-79fa-43ed-bbd9-c13ba7231ff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8883857125251213,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestao",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
